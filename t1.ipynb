{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm as tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing da Página Principal da Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria o diretório para armazenar os verbetes, percorre todo o conjunto e salva o HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_infobox(url):\n",
    "    e = requests.get(url)\n",
    "    soup = BeautifulSoup(e.text, 'html.parser') \n",
    "    tables = soup.findAll('table')\n",
    "    infobox = \"\"\n",
    "    for table in tables:\n",
    "        if \"infobox\" in table[\"class\"] or \"infobox_v2\" in table[\"class\"]:\n",
    "            infobox = table\n",
    "            break\n",
    "\n",
    "    chava = {}\n",
    "#dando erro. Provavelmente por causa da forma como o título está sendo pego - lavie\n",
    "    titulo = infobox.findAll(\"th\")[0]\n",
    "    for tr in soup.findAll('tr'):\n",
    "        if len(tr.findAll(\"td\")) == 2:\n",
    "            messi = tr.findAll(\"td\")\n",
    "            chava[messi[0].text.strip()] = [li.text.strip() for li in messi[1].findAll(\"li\")] if messi[1].findAll(\"li\") else messi[1].text.strip()\n",
    "            \n",
    "    print(chava)\n",
    "    chava[\"Título\"] = titulo.text.strip()\n",
    "    chava_df = pd.DataFrame.from_dict(chava, orient='index')\n",
    "    chava_df.transpose().to_json(f'verbetes/{titulo}/{titulo}.json', orient='index')\n",
    "    print(chava_df.transpose())\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cria_pasta(all_verbets, a):\n",
    "    os.makedirs('verbetes', exist_ok=True)\n",
    "    for verbet in tqdm(all_verbets):\n",
    "        titulo = verbet.split('/')[-1]\n",
    "        soup = BeautifulSoup(a.text, 'html.parser')\n",
    "        os.makedirs(f'verbetes/{titulo}', exist_ok=True)\n",
    "        with open(f'verbetes/{titulo}/{titulo}.html', 'w') as f:\n",
    "            f.write(soup.prettify())\n",
    "        time.sleep(random.randint(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get('https://pt.wikipedia.org/wiki/Wikip%C3%A9dia:P%C3%A1gina_principal')\n",
    "soup = BeautifulSoup(a.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada link na seção \"main\" da página principal, verifica se o link representa um verbete, caso positivo, adiciona à um Set para evitar duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.main.find_all('a')\n",
    "all_verbets = set()\n",
    "for link in links:\n",
    "    if link.has_attr('href') and re.match(r'/wiki/.*', link['href']) and not re.match(r'/wiki/.*:.*', link['href']):\n",
    "            all_verbets.add(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhe aleatoriamente um verbet da conjunto obtido da página principal, e faz o mesmo processamento. O loop é executado até que o conjunto de verbetes tenha 5000 verbetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:55<00:00,  2.06it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m         all_verbets.add(link[\u001b[33m'\u001b[39m\u001b[33mhref\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m         cria_pasta(all_verbets, a)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[43mget_infobox\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://pt.wikipedia.org\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m time.sleep(random.randint(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mget_infobox\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     11\u001b[39m chava = {}\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m titulo = \u001b[43minfobox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindAll\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tr \u001b[38;5;129;01min\u001b[39;00m soup.findAll(\u001b[33m'\u001b[39m\u001b[33mtr\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tr.findAll(\u001b[33m\"\u001b[39m\u001b[33mtd\u001b[39m\u001b[33m\"\u001b[39m)) == \u001b[32m2\u001b[39m:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "while(len(all_verbets) < 115):\n",
    "    verbet = random.choice(list(all_verbets))\n",
    "    a = requests.get('https://pt.wikipedia.org' + verbet)\n",
    "    soup = BeautifulSoup(a.text, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        if link.has_attr('href') and re.match(r'/wiki/.*', link['href']) and not re.match(r'/wiki/.*:.*', link['href']):\n",
    "            all_verbets.add(link['href'])\n",
    "            cria_pasta(all_verbets, a)\n",
    "            get_infobox('https://pt.wikipedia.org' + verbet)\n",
    "            \n",
    "    time.sleep(random.randint(1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpa-e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
